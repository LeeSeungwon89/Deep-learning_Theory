{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9OkUQ/ZelN6mC/DacJWs2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Deep-learning_Theory/blob/main/9-1%20%EC%88%9C%EC%B0%A8%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80%20%EC%88%9C%ED%99%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9-1 순차 데이터와 순환 신경망**"
      ],
      "metadata": {
        "id": "Z4OMQWEWwNz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **순차 데이터**"
      ],
      "metadata": {
        "id": "S1vtRuHhwNxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**순차 데이터(sequential data)**는 텍스트나 **시계열 데이터(time series data)**처럼 순서가 유의미한 데이터를 의미합니다. 이전 챕터에서 다뤘던 데이터들은 순서와 관련이 없었습니다. 오히려 섞어서 활용하여 더 좋은 결과를 도출했습니다. 하지만 순차 데이터는 데이터 자체의 순서가 핵심이므로 순서를 유지하며 신경망에 주입할 필요가 있습니다.\n",
        "\n",
        "순차 데이터를 활용하려면 이전에 입력한 데이터를 기억하는 기능이 필수입니다. 완전 연결 신경망과 합성곱 신경망은 이 기능이 없습니다. 샘플 하나 또는 배치 하나를 사용하여 정방향 계산을 수행하면 이 샘플 또는 배치는 버려지므로 다음 샘플을 처리하는 과정에서 다시 사용하지 않습니다. 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 **피드포워드 신경망(feedforward neural network)**이라고 부릅니다. 이전 챕터에서 다뤘던 완전 연결 신경망과 합성곱 신경망이 바로 피드포워드 신경망입니다."
      ],
      "metadata": {
        "id": "sbeTrYwcNPZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **순환 신경망**"
      ],
      "metadata": {
        "id": "WzkxzvhywNu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 처리했던 샘플을 다음 샘플을 처리하는 과정에서 다시 사용하려면 피드포워드 신경망처럼 데이터 흐름이 앞으로만 전달되지 않아야 합니다. 이전 데이터가 신경망 층에 순환되어야 합니다. 이렇게 순환되는 신경망을 **순환 신경망(recurrent neural network)**이라고 부릅니다. 순환 신경망은 완전 연결 신경망과 비슷하지만 이전 데이터의 처리 흐름을 순환하는 고리가 하나 추가된 차이가 있습니다. 유닛의 출력이 유닛 자신에게 다시 전달되는 것이며, 이는 어떤 샘플을 처리하는 과정에서 이전에 사용한 데이터를 다시 사용하는 것을 의미합니다. 예컨대 A, B, C라는 샘플을 처리하는 순환 신경망의 유닛이 있고(샘플 처리 순도 A, B, C) $O$라는 출력 결과가 있다고 가정하겠습니다.\n",
        "\n",
        "- C B A -> 유닛 -> $O_A$\n",
        "\n",
        "A를 처리한 출력 결과인 $O_A$는 A에 대한 정보를 가집니다. 이 출력 결과는 다시 유닛으로 들어가며, B를 처리할 때 이 출력 결과를 같이 사용합니다.\n",
        "\n",
        "- C B -> 유닛 -> $O_B$\n",
        "\n",
        "B를 처리한 출력 결과인 $O_B$는 A와 B에 대한 정보를 가집니다. 이 출력 결과는 다시 유닛으로 들어가며, C를 처리할 때 이 출력 결과를 같이 사용합니다.\n",
        "\n",
        "- C -> 유닛 -> $O_C$\n",
        "\n",
        "C를 처리한 출력 결과인 $O_C$는 A와 B, C에 대한 정보를 모두 가집니다. 다만 이 출력 결과에는 A에 대한 정보보다는 B에 대한 정보가 더 많습니다. 순환 신경망은 이전 샘플에 대한 기억을 더 많이 가지기 때문입니다. 위 과정을 **타임스텝(timestep)**이라고 합니다. 즉, 순환 신경망은 이전 타임스텝 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보가 희미해집니다.\n",
        "\n",
        "순환 신경망의 층은 **셀(cell)**이라고 부릅니다. 한 셀에는 여러 유닛이 존재하지만 완전 연결 신경망과 달리 유닛을 모두 표시하지 않고 셀 하나로 층을 표현합니다. 아울러 셀의 출력을 **은닉 상태(hidden state)**라고 부릅니다. 물론 신경망 구조마다 부르는 명칭은 다를 수 있지만 기본 구조는 같습니다. 입력에 가중치를 곱하고 절편을 더하며 활성화 함수를 통과시키고 다음 층으로 보내는 프로세스입니다. 유일한 차이점은 은닉 상태(층의 출력)을 다음 타임 스텝에 다시 사용하는 것입니다. 은닉 상태의 크기는 셀에 있는 유닛의 개수와 같습니다.\n",
        "\n",
        "바로 위에서 예시한 과정을 다시 예로 들어 정리하겠습니다.\n",
        "\n",
        "- C B A -> 유닛(셀) -> 활성화 함수 -> $O_A$(은닉 상태)\n",
        "\n",
        "유닛은 셀이고, 활성화 함수를 거쳐 은닉 상태가 됩니다. 이 은닉 상태는 다시 셀로 돌아가 재사용됩니다. \n",
        "\n",
        "활성화 함수는 **하이퍼볼릭 탄젠트(hyperbolic tangent)** 함수인 **tanh2**가 많이 사용됩니다. tanh 함수가 S자 모양을 띠므로 시그모이드 함수라고 부를 수 있지만, 기실 시그모이드 함수의 범위는 0 ~ 1인 반면 tanh 함수의 범위는 -1 ~ 1입니다.\n",
        "\n",
        "합성곱 신경망 같은 피드포워드 신경망에서 유닛은 입력과 가중치를 곱합니다. 이처럼 순환 신경망에서도 입력과 가중치를 곱합니다. 그러나 순환 신경망의 유닛은 가중치 하나를 더 가집니다. 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다. 셀은 입력과 이전 타임스텝의 은닉 상태를 재사용하여 현재 타임스텝의 은닉 상태를 업데이트합니다. 이를 예시해보겠습니다. $W$는 가중치, $h$는 은닉 상태입니다. $W_X$는 입력에 곱해지는 가중치, $W_h$는 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다.\n",
        "\n",
        "- 셀($W_X$가 곱해짐) -> 활성화 함수 -> $h$\n",
        "\n",
        "이후 $h$가 셀에서 재사용될 때 $W_h$가 추가로 곱해집니다. 이 과정을 여러 타임스텝으로 셀을 나누어(타임스텝을 펼쳐) 예시해 보겠습니다. 타임스텝 1, 타임스텝 2처럼 여러 타임스텝이 존재한다고 가정하겠습니다.\n",
        "\n",
        "- 타임스텝 1: 셀($W_X$가 곱해짐) -> 활성화 함수 -> $h_1$\n",
        "\n",
        "- 타임스텝 2: 셀($W_X$과 $W_h$ 곱해짐) -> 활성화 함수 -> $h_2$\n",
        "\n",
        "- 타임스텝 3: 셀($W_X$과 $W_h$ 곱해짐) -> 활성화 함수 -> $h_3$\n",
        "\n",
        "은닉 상태는 $h_1, h_2, h_3$처럼 여러 개지만(실제로는 마지막 타임스텝의 $h_3$만 출력됨) 가중치 $W_h$는 하나이고, 이 가중치가 모든 타임스텝에 공통적으로 곱해지는 점을 알 수 있습니다. 가중치 $W_h$는 타임스텝에 따라 변화되는 유닛의 출력을 학습합니다. "
      ],
      "metadata": {
        "id": "dOkO2EvzdCK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **셀의 가중치와 입출력**"
      ],
      "metadata": {
        "id": "cDmgKh1CwNsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예시로써 순환 신경망의 셀에서 필요한 가중치 크기를 구해 보겠습니다. 순환층에 입력되는 특성 개수가 4개($X_1, X_2, X_3, X_4$), 순환층의 유닛이 3개($r_1, r_2, r_3$)라고 가정하겠습니다. 각 특성은 유닛 3개에 완전 연결되므로 가중치 $W_X$의 크기는 $4 \\times 3 = 12$개입니다.\n",
        "\n",
        "다음으로 순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 $W_h$의 크기를 구해 보겠습니다. $r1$의 은닉 상태가 다음 타임스텝에 재사용될 때 $r1$, $r2$, $r3$에 모두 전달됩니다. 이전 타임스텝의 은닉 상태가 다음 타임스텝의 뉴런에 완전히 연결되는 것입니다. $r2$의 은닉 상태 또한 세 유닛에 모두 전달되고 $r3$의 은닉 상태 또한 이와 같습니다. 따라서 가중치 $W_h$의 크기는 $3 \\times 3 = 9$개입니다.\n",
        "\n",
        "모델 파라미터 개수도 구해 보겠습니다. 각 유닛마다 절편 하나를 가지며, 모델 파라미터 개수는 가중치 개수 + 절편 개수이므로 $12 + 9 + 3 = 24$개입니다.\n",
        "\n",
        "순환층의 입력과 출력에 대해 살펴 보겠습니다. 이전 챕터에서의 합성곱 층의 입력은 전형적으로 샘플 하나가 차원 3개(너비, 높이, 채널(또는 깊이))를 가졌습니다. 입력이 합성곱 층과 풀링 층을 통과하면 너비, 높이, 채널(또는 깊이)의 크기가 달라지지만 차원 개수는 그대로 유지됐습니다. 반면에 순환층은 일반적으로 샘플마다 차원 2개를 가집니다. 샘플 하나를 시퀀스(sequence) 하나라고 부릅니다. 시퀀스 내에는 여러 아이템이 들었고, 시퀀스 길이는 타임스텝 길이와 같습니다. 예컨대 'I'am a boy.'라는 샘플의 경우 단어 4개(I, am, a, boy)로 구성됩니다. 각 단어를 어떤 숫자 3개로 표현한다고 가정하면 타임스텝 크기는 (1, 4, 3)이 됩니다. 1은 샘플 개수, 4는 단어 개수, 3은 어떤 숫자 3개를 의미합니다. 이 입력이 순환층을 통과하면 두 세 번째 차원인 4와 3이 사라지고 순환층의 유닛 개수만큼 출력됩니다. 샘플 하나는 시퀀스 길이(단어 개수)와 단어 표현(다음 챕터에서 다룸)의 2차원 배열입니다. 순환층을 통과하면 1차원 배열로 바뀌며 배열 크기는 순환층의 유닛 개수에 의해 결정됩니다.\n",
        "\n",
        "위에서 명시했듯이 순환층은 마지막 타임스텝의 은닉 상태만 출력으로 내보냅니다. 시퀀스 길이를 모두 읽고 정보를 마지막 은닉 상태에 압축하여 전달하는 것이며, 순환 신경망이 정보를 기억하는 메모리를 가지는 것입니다.\n",
        "\n",
        "순환 신경망도 여러 층을 쌓을 수 있습니다. 셀의 입력은 샘플마다 타임스텝과 단어 표현으로 이루어진 2차원 배열이어야 합니다. 셀이 2개라고 가정할 때 첫 번째 셀에서는 모든 타임스텝의 정보를 가진 은닉 상태를 출력하고, 두 번째 셀이 이 은닉 상태를 받아 내부에서 은닉 상태들을 재사용한 뒤 마지막 타임스텝의 은닉 상태만 출력합니다. 구체적인 내용은 다음 챕터에서 다루겠습니다.\n",
        "\n",
        "순환 신경망 또한 다른 신경망처럼 마지막에 밀집층을 구성하여 클래스를 분류합니다. 다중 분류이면 출력층에 클래스 개수만큼 유닛을 두고 소프트맥스 활성화 함수를 사용합니다. 이진 분류이면 유닛 하나를 두고 시그모이드 활성화 함수를 사용합니다. 다만 합성곱 신경망과 달리 마지막 셀의 출력이 1차원이므로 차원을 펼칠 필요가 없습니다. 셀의 출력을 밀집층에 바로 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "MCSHWWkozi9K"
      }
    }
  ]
}