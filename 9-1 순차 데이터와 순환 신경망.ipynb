{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzd+KEQcLm+9uyBZ58EUNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeSeungwon89/Deep-learning_Theory/blob/main/9-1%20%EC%88%9C%EC%B0%A8%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80%20%EC%88%9C%ED%99%98%20%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9-1 순차 데이터와 순환 신경망**"
      ],
      "metadata": {
        "id": "Z4OMQWEWwNz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **순차 데이터**"
      ],
      "metadata": {
        "id": "S1vtRuHhwNxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**순차 데이터(sequential data)**는 텍스트나 **시계열 데이터(time series data)**처럼 순서가 유의미한 데이터를 의미합니다. 이전 챕터에서 다뤘던 데이터들은 순서와 관련이 없었습니다. 오히려 섞어서 활용하여 더 좋은 결과를 도출했습니다. 하지만 순차 데이터는 데이터 자체의 순서가 핵심이므로 순서를 유지하며 신경망에 주입할 필요가 있습니다.\n",
        "\n",
        "순차 데이터를 활용하려면 이전에 입력한 데이터를 기억하는 기능이 필수입니다. 완전 연결 신경망과 합성곱 신경망은 이 기능이 없습니다. 샘플 하나 또는 배치 하나를 사용하여 정방향 계산을 수행하면 이 샘플 또는 배치는 버려지므로 다음 샘플을 처리하는 과정에서 다시 사용하지 않습니다. 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 **피드포워드 신경망(feedforward neural network)**이라고 부릅니다. 이전 챕터에서 다뤘던 완전 연결 신경망과 합성곱 신경망이 바로 피드포워드 신경망입니다."
      ],
      "metadata": {
        "id": "sbeTrYwcNPZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **순환 신경망**"
      ],
      "metadata": {
        "id": "WzkxzvhywNu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 처리했던 샘플을 다음 샘플을 처리하는 과정에서 다시 사용하려면 피드포워드 신경망처럼 데이터 흐름이 앞으로만 전달되지 않아야 합니다. 이전 데이터가 신경망 층에 순환되어야 합니다. 이렇게 순환되는 신경망을 **순환 신경망(recurrent neural network)**이라고 부릅니다. 순환 신경망은 완전 연결 신경망과 비슷하지만 이전 데이터의 처리 흐름을 순환하는 고리가 하나 추가된 차이가 있습니다. 유닛의 출력이 유닛 자신에게 다시 전달되는 것이며, 이는 어떤 샘플을 처리하는 과정에서 이전에 사용한 데이터를 다시 사용하는 것을 의미합니다. 예컨대 A, B, C라는 샘플을 처리하는 순환 신경망의 유닛이 있고(샘플 처리 순도 A, B, C) $O$라는 출력 결과가 있다고 가정하겠습니다.\n",
        "\n",
        "- C B A -> 유닛 -> $O_A$\n",
        "\n",
        "A를 처리한 출력 결과인 $O_A$는 A에 대한 정보를 가집니다. 이 출력 결과는 다시 유닛으로 들어가며, B를 처리할 때 이 출력 결과를 같이 사용합니다.\n",
        "\n",
        "- C B -> 유닛 -> $O_B$\n",
        "\n",
        "B를 처리한 출력 결과인 $O_B$는 A와 B에 대한 정보를 가집니다. 이 출력 결과는 다시 유닛으로 들어가며, C를 처리할 때 이 출력 결과를 같이 사용합니다.\n",
        "\n",
        "- C -> 유닛 -> $O_C$\n",
        "\n",
        "C를 처리한 출력 결과인 $O_C$는 A와 B, C에 대한 정보를 모두 가집니다. 다만 이 출력 결과에는 A에 대한 정보보다는 B에 대한 정보가 더 많습니다. 순환 신경망은 이전 샘플에 대한 기억을 더 많이 가지기 때문입니다. 위 과정을 **타임스텝(timestep)**이라고 합니다. 즉, 순환 신경망은 이전 타임스텝 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보가 희미해집니다.\n",
        "\n",
        "순환 신경망의 층은 **셀(cell)**이라고 부릅니다. 한 셀에는 여러 유닛이 존재하지만 완전 연결 신경망과 달리 유닛을 모두 표시하지 않고 셀 하나로 층을 표현합니다. 아울러 셀의 출력을 **은닉 상태(hidden state)**라고 부릅니다. 물론 신경망 구조마다 부르는 명칭은 다를 수 있지만 기본 구조는 같습니다. 입력에 가중치를 곱하고 절편을 더하며 활성화 함수를 통과시키고 다음 층으로 보내는 프로세스입니다. 유일한 차이점은 은닉 상태(층의 출력)을 다음 타임 스텝에 다시 사용하는 것입니다.\n",
        "\n",
        "바로 위에서 예시한 과정을 다시 예로 들어 정리하겠습니다.\n",
        "\n",
        "- C B A -> 유닛(셀) -> 활성화 함수 -> $O_A$(은닉 상태)\n",
        "\n",
        "유닛은 셀이고, 활성화 함수를 거쳐 은닉 상태가 됩니다. 이 은닉 상태는 다시 셀로 돌아가 재사용됩니다. \n",
        "\n",
        "활성화 함수는 **하이퍼볼릭 탄젠트(hyperbolic tangent)** 함수인 **tanh2**가 많이 사용됩니다. tanh 함수가 S자 모양을 띠므로 시그모이드 함수라고 부를 수 있지만, 기실 시그모이드 함수의 범위는 0 ~ 1인 반면 tanh 함수의 범위는 -1 ~ 1입니다.\n",
        "\n",
        "합성곱 신경망 같은 피드포워드 신경망에서 유닛은 입력과 가중치를 곱합니다. 이처럼 순환 신경망에서도 입력과 가중치를 곱합니다. 그러나 순환 신경망의 유닛은 가중치 하나를 더 가집니다. 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다. 셀은 입력과 이전 타임스텝의 은닉 상태를 재사용하여 현재 타임스텝의 은닉 상태를 업데이트합니다. 이를 예시해보겠습니다. $W$는 가중치, $h$는 은닉 상태입니다. $W_X$는 입력에 곱해지는 가중치, $W_h$는 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다.\n",
        "\n",
        "- 셀($W_X$가 곱해짐) -> 활성화 함수 -> $h$\n",
        "\n",
        "이후 $h$가 셀에서 재사용될 때 $W_h$가 추가로 곱해집니다. 이 과정을 여러 타임스텝으로 셀을 나누어(타임스텝을 펼쳐) 예시해 보겠습니다. 타임스텝 1, 타임스텝 2처럼 여러 타임스텝이 존재한다고 가정하겠습니다.\n",
        "\n",
        "- 타임스텝 1: 셀($W_X$가 곱해짐) -> 활성화 함수 -> $h_1$\n",
        "\n",
        "- 타임스텝 2: 셀($W_X$과 $W_h$ 곱해짐) -> 활성화 함수 -> $h_2$\n",
        "\n",
        "- 타임스텝 3: 셀($W_X$과 $W_h$ 곱해짐) -> 활성화 함수 -> $h_3$\n",
        "\n",
        "은닉 상태는 $h_1, h_2, h_3$처럼 여러 개지만 가중치 $W_h$는 하나이고, 이 가중치가 모든 타임스텝에 공통적으로 곱해지는 점을 알 수 있습니다. 가중치 $W_h$는 타임스텝에 따라 변화되는 유닛의 출력을 학습합니다. "
      ],
      "metadata": {
        "id": "dOkO2EvzdCK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **셀의 가중치와 입출력**"
      ],
      "metadata": {
        "id": "cDmgKh1CwNsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "작업 중\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pTn3BCHxzHMO"
      }
    }
  ]
}